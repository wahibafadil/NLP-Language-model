{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### La 1 ère partie : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "class NgramLanguageModel:\n",
    "    def __init__(self): \n",
    "        self.trigram_counts = defaultdict(int)\n",
    "        self.bigram_counts = defaultdict(int)\n",
    "        self.word_counts = Counter()\n",
    "        self.k = 0.01\n",
    "\n",
    "    def count_words(self, tokenized_sentences):\n",
    "        \"\"\"\n",
    "        Count the frequency of each word in a list of tokenized sentences.\n",
    "        \"\"\"\n",
    "        word_counter = Counter()\n",
    "        for sentence in tokenized_sentences:\n",
    "            word_counter.update(sentence)\n",
    "        return word_counter\n",
    "\n",
    "    def get_words_with_nplus_frequency(self, tokenized_sentences, count_threshold):\n",
    "        \"\"\"\n",
    "        Get the list of words with at least 'count_threshold' frequency.\n",
    "        \"\"\"\n",
    "        word_counts = self.count_words(tokenized_sentences)\n",
    "        closed_vocab = [word for word, count in word_counts.items() if count >= count_threshold]\n",
    "        return closed_vocab\n",
    "    \n",
    "    def replace_oov_words_by_unk(self, tokenized_sentences, vocabulary, unknown_token=\"<unk>\"):\n",
    "        \"\"\"\n",
    "        Replace words not in the vocabulary with the given unknown token.\n",
    "        \"\"\"\n",
    "        vocabulary_set = set(vocabulary)\n",
    "        replaced_tokenized_sentences = []\n",
    "        for sentence in tokenized_sentences:\n",
    "            replaced_sentence = [\n",
    "                token if token in vocabulary_set else unknown_token\n",
    "                for token in sentence\n",
    "            ]\n",
    "            replaced_tokenized_sentences.append(replaced_sentence)\n",
    "        return replaced_tokenized_sentences\n",
    "\n",
    "    def prepare_data(self, infile, ngram_size=2):\n",
    "\n",
    "        # Lire le contenu du fichier\n",
    "        with open(infile, 'r', encoding='utf-8') as f:\n",
    "            raw_text = f.read().strip().lower()\n",
    "\n",
    "        # Séparer le texte par lignes\n",
    "        raw_lines = raw_text.split('\\n')\n",
    "\n",
    "        # Tokeniser chaque ligne en mots tout en gardant la ponctuation\n",
    "        tokenized_lines = [re.findall(r'\\w+|[.!?]', line) for line in raw_lines]\n",
    "\n",
    "        # Définir le nombre minimum d'occurrences pour garder un mot\n",
    "        min_occurrences = 2\n",
    "\n",
    "        # Obtenir le vocabulaire des mots fréquents\n",
    "        vocabulary = self.get_words_with_nplus_frequency(tokenized_lines, min_occurrences)\n",
    "\n",
    "        # Remplacer les mots OOV par <unk>\n",
    "        tokenized_lines = self.replace_oov_words_by_unk(tokenized_lines, vocabulary)\n",
    "\n",
    "        # Ajouter des jetons de début et de fin de phrase\n",
    "        preprocessed_sentences = []\n",
    "        for tokens in tokenized_lines:\n",
    "            if ngram_size == 2:\n",
    "                preprocessed_sentences.append(['<s>'] + tokens + ['</s>'])\n",
    "            elif ngram_size == 3:\n",
    "                preprocessed_sentences.append(['<s>', '<s>'] + tokens + ['</s>'])\n",
    "\n",
    "        # Joindre toutes les phrases pour obtenir le corpus prétraité\n",
    "        preprocessed_corpus = ' '.join([' '.join(tokens) for tokens in preprocessed_sentences])\n",
    "\n",
    "        return preprocessed_corpus\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def train(self, infile, ngram_size=2):\n",
    "\n",
    "        \n",
    "        preprocessed_corpus = self.prepare_data(infile, ngram_size)\n",
    "\n",
    "        # Tokeniser le corpus en mots\n",
    "        tokens = preprocessed_corpus.split()\n",
    "\n",
    "        if ngram_size == 2:\n",
    "            # Calculer les bigrams\n",
    "            for i in range(len(tokens) - 1):\n",
    "                bigram = (tokens[i], tokens[i + 1])\n",
    "                self.bigram_counts[bigram] += 1\n",
    "                self.word_counts[tokens[i]] += 1\n",
    "\n",
    "            # Compter le dernier mot comme précédent pour le bigram final\n",
    "            self.word_counts[tokens[-1]] += 1\n",
    "\n",
    "        elif ngram_size == 3:\n",
    "            # Calculer les trigrams\n",
    "            for i in range(len(tokens) - 2):\n",
    "                trigram = (tokens[i], tokens[i + 1], tokens[i + 2])\n",
    "                self.trigram_counts[trigram] += 1\n",
    "            \n",
    "            # Compter le dernier bigram comme précédent pour le trigram final\n",
    "            self.word_counts[tokens[-2]] += 1\n",
    "            self.word_counts[tokens[-1]] += 1\n",
    "        \n",
    "        \n",
    "        # Lissage pour les bigrams\n",
    "        self.bigram_probabilities = defaultdict(float)\n",
    "        for bigram, count in self.bigram_counts.items():\n",
    "            word_1 = bigram[0]\n",
    "            probability = (count + self.k) / (self.word_counts[word_1] + self.k * len(self.word_counts))\n",
    "            self.bigram_probabilities[bigram] = math.log(probability)\n",
    "        \n",
    "        # Lissage pour les trigrams\n",
    "        self.trigram_probabilities = defaultdict(float)\n",
    "        for trigram, count in self.trigram_counts.items():\n",
    "            bigram = (trigram[0], trigram[1])\n",
    "            probability = (count + self.k) / (self.bigram_counts.get(bigram,0) + self.k * len(self.word_counts))\n",
    "            self.trigram_probabilities[trigram] = math.log(probability)\n",
    "\n",
    "\n",
    "    def predict_ngram(self, sentence, ngram_size=2):\n",
    "        # Préparer la phrase avec la méthode `prepare_data`\n",
    "        preprocessed_corpus = self.prepare_data(sentence, ngram_size)\n",
    "\n",
    "        # Tokeniser le corpus prétraité en mots\n",
    "        tokens = preprocessed_corpus.split()\n",
    "\n",
    "        # Initialiser la probabilité totale\n",
    "        total_log_prob = 0.0\n",
    "\n",
    "        if ngram_size == 2:\n",
    "            # Calculer les probabilités des bigrams\n",
    "            for i in range(len(tokens) - 1):\n",
    "                bigram = (tokens[i], tokens[i + 1])\n",
    "\n",
    "                # Obtenir la probabilité du bigram\n",
    "                if bigram in self.bigram_probabilities:\n",
    "                    # Ajouter la probabilité logarithmique du bigram\n",
    "                    total_log_prob += self.bigram_probabilities[bigram]\n",
    "                else:\n",
    "                    # Si le bigram n'est pas trouvé, appliquer un lissage supplémentaire\n",
    "                    vocabulary_size = len(self.word_counts)\n",
    "                    probability = self.k / (self.k * vocabulary_size)\n",
    "                    total_log_prob += math.log(probability)\n",
    "\n",
    "        elif ngram_size == 3:\n",
    "            # Calculer les probabilités des trigrams\n",
    "            for i in range(len(tokens) - 2):\n",
    "                trigram = (tokens[i], tokens[i + 1], tokens[i + 2])\n",
    "\n",
    "                # Obtenir la probabilité du trigram\n",
    "                if trigram in self.trigram_probabilities:\n",
    "                    # Ajouter la probabilité logarithmique du trigram\n",
    "                    total_log_prob += self.trigram_probabilities[trigram]\n",
    "                else:\n",
    "                    # Si le trigram n'est pas trouvé, appliquer un lissage supplémentaire\n",
    "                    vocabulary_size = len(self.word_counts)\n",
    "                    probability = self.k / (self.k * vocabulary_size)\n",
    "                    total_log_prob += math.log(probability)\n",
    "\n",
    "        return total_log_prob\n",
    "    \n",
    "\n",
    "    def test_perplexity(self, test_file, ngram_size=2):\n",
    "        \"\"\"\n",
    "        Calcule la perplexité du modèle entraîné sur un corpus de test.\n",
    "        \"\"\"\n",
    "\n",
    "        # Prétraiter le corpus de test\n",
    "        preprocessed_corpus = self.prepare_data(test_file, ngram_size)\n",
    "\n",
    "        # Diviser le corpus en phrases\n",
    "        sentences = preprocessed_corpus.split('</s>')\n",
    "\n",
    "        # Calculer la probabilité logarithmique totale\n",
    "        total_log_prob = 0.0\n",
    "        total_tokens = 0\n",
    "\n",
    "        # Pour chaque phrase du corpus de test\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()  # Nettoyer les espaces\n",
    "\n",
    "            if sentence:  # Si la phrase n'est pas vide\n",
    "                # Calculez la probabilité logarithmique de la phrase\n",
    "                log_prob = self.predict_ngram(sentence, ngram_size)\n",
    "                total_log_prob += log_prob\n",
    "                \n",
    "                # Compter le nombre total de tokens, y compris le token de fin de phrase\n",
    "                num_tokens_in_sentence = len(sentence.split()) + 1\n",
    "                total_tokens += num_tokens_in_sentence\n",
    "\n",
    "        # Normaliser le total des log probabilities par le nombre total de tokens\n",
    "        normalized_log_prob = total_log_prob / total_tokens\n",
    "        \n",
    "        # Calculer la perplexité : exp(-normalized_log_prob)\n",
    "        perplexity = math.exp(-normalized_log_prob)\n",
    "\n",
    "        return perplexity\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus prétraité avec Bigram:\n",
      "<s> the quick <unk> fox jumps over the lazy dog . </s> <s> the quick <unk> fox jumps over the lazy dog . </s>\n",
      "Corpus prétraité avec Trigram:\n",
      "<s> <s> the quick <unk> fox jumps over the lazy dog . </s> <s> <s> the quick <unk> fox jumps over the lazy dog . </s>\n"
     ]
    }
   ],
   "source": [
    "#Testing prepare_data fct \n",
    "ngram_model = NgramLanguageModel()\n",
    "\n",
    "bigram = ngram_model.prepare_data('test_corpus.txt', ngram_size=2)\n",
    "\n",
    "print(\"Corpus prétraité avec Bigram:\")\n",
    "print(bigram)\n",
    "\n",
    "# Utilisez la méthode 'prepare_data' pour prétraiter le corpus comme trigram\n",
    "trigram = ngram_model.prepare_data('test_corpus.txt', ngram_size=3)\n",
    "\n",
    "print(\"Corpus prétraité avec Trigram:\")\n",
    "print( trigram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comptes de bigrams:\n",
      "('<s>', 'the'): 2\n",
      "('the', 'quick'): 2\n",
      "('quick', '<unk>'): 2\n",
      "('<unk>', 'fox'): 2\n",
      "('fox', 'jumps'): 2\n",
      "('jumps', 'over'): 2\n",
      "('over', 'the'): 2\n",
      "('the', 'lazy'): 2\n",
      "('lazy', 'dog'): 2\n",
      "('dog', '.'): 2\n",
      "('.', '</s>'): 2\n",
      "('</s>', '<s>'): 1\n"
     ]
    }
   ],
   "source": [
    "# Testing train fct\n",
    "\n",
    "ngram_model = NgramLanguageModel()\n",
    "\n",
    "ngram_model.train('test_corpus.txt', ngram_size=2)\n",
    "\n",
    "# Afficher quelques bigram counts\n",
    "print(\"Comptes de bigrams:\")\n",
    "for bigram, count in list(ngram_model.bigram_counts.items()):\n",
    "    print(f\"{bigram}: {count}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Probabilités de bigrams:\n",
      "('<s>', 'the'): -0.04855\n",
      "('the', 'quick'): -0.71529\n",
      "('quick', '<unk>'): -0.04855\n",
      "('<unk>', 'fox'): -0.04855\n",
      "('fox', 'jumps'): -0.04855\n",
      "('jumps', 'over'): -0.04855\n",
      "('over', 'the'): -0.04855\n",
      "('the', 'lazy'): -0.71529\n",
      "('lazy', 'dog'): -0.04855\n",
      "('dog', '.'): -0.04855\n",
      "('.', '</s>'): -0.04855\n",
      "('</s>', '<s>'): -0.73674\n"
     ]
    }
   ],
   "source": [
    "# Afficher quelques probabilités de bigrams\n",
    "print(\"\\nProbabilités de bigrams:\")\n",
    "for bigram, prob in list(ngram_model.bigram_probabilities.items()):\n",
    "    print(f\"{bigram}: {prob:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comptes de trigrams:\n",
      "('<s>', '<s>', 'the'): 2\n",
      "('<s>', 'the', 'quick'): 2\n",
      "('the', 'quick', '<unk>'): 2\n",
      "('quick', '<unk>', 'fox'): 2\n",
      "('<unk>', 'fox', 'jumps'): 2\n",
      "('fox', 'jumps', 'over'): 2\n",
      "('jumps', 'over', 'the'): 2\n",
      "('over', 'the', 'lazy'): 2\n",
      "('the', 'lazy', 'dog'): 2\n",
      "('lazy', 'dog', '.'): 2\n",
      "('dog', '.', '</s>'): 2\n",
      "('.', '</s>', '<s>'): 1\n",
      "('</s>', '<s>', '<s>'): 1\n"
     ]
    }
   ],
   "source": [
    "# Entraînez le modèle avec des trigrams\n",
    "ngram_model.train('', ngram_size=3)\n",
    "\n",
    "# Afficher quelques trigram counts\n",
    "print(\"\\nComptes de trigrams:\")\n",
    "for trigram, count in list(ngram_model.trigram_counts.items()):\n",
    "    print(f\"{trigram}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Probabilités de trigrams:\n",
      "('<s>', '<s>', 'the'): 2.90541\n",
      "('<s>', 'the', 'quick'): -0.04855\n",
      "('the', 'quick', '<unk>'): -0.04855\n",
      "('quick', '<unk>', 'fox'): -0.04855\n",
      "('<unk>', 'fox', 'jumps'): -0.04855\n",
      "('fox', 'jumps', 'over'): -0.04855\n",
      "('jumps', 'over', 'the'): -0.04855\n",
      "('over', 'the', 'lazy'): -0.04855\n",
      "('the', 'lazy', 'dog'): -0.04855\n",
      "('lazy', 'dog', '.'): -0.04855\n",
      "('dog', '.', '</s>'): -0.04855\n",
      "('.', '</s>', '<s>'): -0.73674\n",
      "('</s>', '<s>', '<s>'): -0.09441\n"
     ]
    }
   ],
   "source": [
    "# Afficher quelques probabilités de trigrams\n",
    "print(\"\\nProbabilités de trigrams:\")\n",
    "for trigram, prob in list(ngram_model.trigram_probabilities.items()):\n",
    "    print(f\"{trigram}: {prob:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilité logarithmique de la phrase 'test_corpus.txt': -4.471848899889278\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ngram_model = NgramLanguageModel()\n",
    "ngram_model.train('test_corpus.txt', ngram_size=2)\n",
    "log_prob = ngram_model.predict_ngram('test_corpus.txt', ngram_size=2)\n",
    "\n",
    "print(f\"Probabilité logarithmique de la phrase '{'test_corpus.txt'}': {log_prob}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
