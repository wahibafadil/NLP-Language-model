{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0966a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world this is an example of ngram text generation\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "# Tokenization and Preprocessing Functions\n",
    "def process_data(data: str) -> Tuple[List[str], List[List[str]]]:\n",
    "    \"\"\"\n",
    "    Tokenize the input data into sentences and words.\n",
    "    \"\"\"\n",
    "    def tokenizer_sentence(sentence: str) -> List[str]:\n",
    "        sentence = sentence.lower()\n",
    "        sentence = re.sub(r\"[^\\w\\s]\", \"\", sentence)\n",
    "        return sentence.split()\n",
    "\n",
    "    sentences = data.split('\\n')\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    tokenized_sentences = [tokenizer_sentence(sentence) for sentence in sentences]\n",
    "    \n",
    "    return sentences, tokenized_sentences\n",
    "\n",
    "def count_words(tokenized_sentences: List[List[str]]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Count occurrences of each word in tokenized sentences.\n",
    "    \"\"\"\n",
    "    word_counts = defaultdict(int)\n",
    "    for sentence in tokenized_sentences:\n",
    "        for token in sentence:\n",
    "            word_counts[token] += 1\n",
    "    return word_counts\n",
    "\n",
    "def get_words_with_frequency_above_or_equal(tokenized_sentences: List[List[str]], threshold: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Retrieve words that appear at least a certain number of times.\n",
    "    \"\"\"\n",
    "    word_counts = count_words(tokenized_sentences)\n",
    "    return [word for word, count in word_counts.items() if count >= threshold]\n",
    "\n",
    "def replace_unknown_words_with_unk(tokenized_sentences: List[List[str]], closed_vocab: List[str], unk_token=\"<unk>\") -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Replace unknown words with a special token.\n",
    "    \"\"\"\n",
    "    closed_vocab_set = set(closed_vocab)\n",
    "    return [[token if token in closed_vocab_set else unk_token for token in sentence] for sentence in tokenized_sentences]\n",
    "\n",
    "def count_n_grams(data: List[List[str]], n: int, start_token: str='<s>', end_token: str='</s>') -> Dict[Tuple[str, ...], int]:\n",
    "    \"\"\"\n",
    "    Count all n-grams in the provided data.\n",
    "    \"\"\"\n",
    "    n_grams = defaultdict(int)\n",
    "    for sentence in data:\n",
    "        sentence = [start_token] * (n-1) + sentence + [end_token]\n",
    "        for i in range(len(sentence) - n + 1):\n",
    "            n_gram = tuple(sentence[i:i+n])\n",
    "            n_grams[n_gram] += 1\n",
    "    return n_grams\n",
    "\n",
    "def train_ngram_model(tokenized_sentences: List[List[str]], n: int, k: float) -> Dict[Tuple[str, ...], float]:\n",
    "    \"\"\"\n",
    "    Train an n-gram model with add-k smoothing.\n",
    "    \"\"\"\n",
    "    n_grams = count_n_grams(tokenized_sentences, n)\n",
    "    vocab = set(token for sentence in tokenized_sentences for token in sentence)\n",
    "    vocab_size = len(vocab)\n",
    "    ngram_probs = {}\n",
    "    for ngram, count in n_grams.items():\n",
    "        context = ngram[:-1]\n",
    "        word = ngram[-1]\n",
    "        context_count = sum(n_grams[context_ngram] for context_ngram in n_grams if context_ngram[:-1] == context)\n",
    "        prob = (count + k) / (context_count + k * vocab_size)\n",
    "        ngram_probs[ngram] = prob\n",
    "    return ngram_probs\n",
    "\n",
    "def predict_ngram(sentence: str, ngram_model: Dict[Tuple[str, ...], float], n: int) -> float:\n",
    "    \"\"\"\n",
    "    Predict the probability of a sentence using an n-gram model.\n",
    "    \"\"\"\n",
    "    sentence = process_data(sentence)[1][0]  # Tokenize sentence\n",
    "    sentence = ['<s>'] * (n - 1) + sentence + ['</s>']\n",
    "    total_log_prob = 0.0\n",
    "    for i in range(n - 1, len(sentence)):\n",
    "        n_gram = tuple(sentence[i - n + 1:i + 1])\n",
    "        total_log_prob += math.log(ngram_model.get(n_gram, 1e-10))  # Use a very small probability if n-gram is not found\n",
    "    return total_log_prob\n",
    "\n",
    "def generate_text(ngram_model: Dict[Tuple[str, ...], float], n: int, max_words: int = 100) -> str:\n",
    "    \"\"\"\n",
    "    Generate text using an n-gram model.\n",
    "    \"\"\"\n",
    "    current_ngram = ('<s>',) * (n - 1)\n",
    "    result = []\n",
    "    for _ in range(max_words):\n",
    "        possible_words = [(ngram[-1], prob) for ngram, prob in ngram_model.items() if ngram[:-1] == current_ngram]\n",
    "        if not possible_words:\n",
    "            break\n",
    "        words, probs = zip(*possible_words)\n",
    "        next_word = np.random.choice(words, p=np.array(probs) / sum(probs))\n",
    "        if next_word == '</s>':\n",
    "            break\n",
    "        result.append(next_word)\n",
    "        current_ngram = current_ngram[1:] + (next_word,)\n",
    "    return ' '.join(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0f18487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('<s>', 'how'): 280, ('how', 'are'): 83, ('are', 'you?'): 44, ('you?', 'btw'): 1, ('btw', 'thanks'): 2, ('thanks', 'for'): 845, ('for', 'the'): 1461, ('the', 'rt.'): 12, ('rt.', 'you'): 1, ('you', 'gonna'): 18, ('gonna', 'be'): 103, ('be', 'in'): 114, ('in', 'dc'): 11, ('dc', 'anytime'): 1, ('anytime', 'soon?'): 4, ('soon?', 'love'): 1, ('love', 'to'): 126, ('to', 'see'): 507, ('see', 'you.'): 6, ('you.', 'been'): 1, ('been', 'way,'): 1, ('way,', 'way'): 1, ('way', 'too'): 33, ('too', 'long.'): 6, ('long.', '</s>'): 10, ('<s>', 'when'): 265, ('when', 'you'): 281, ('you', 'meet'): 6, ('meet', 'someone'): 3, ('someone', 'special...'): 1, ('special...', \"you'll\"): 1, (\"you'll\", 'know.'): 1, ('know.', 'your'): 1, ('your', 'heart'): 20, ('heart', 'will'): 3, ('will', 'beat'): 3, ('beat', 'more'): 2, ('more', 'rapidly'): 2, ('rapidly', 'and'): 2, ('and', \"you'll\"): 24, (\"you'll\", 'smile'): 2, ('smile', 'for'): 3, ('for', 'no'): 10, ('no', 'reason.'): 3, ('reason.', '</s>'): 5, ('<s>', \"they've\"): 3, (\"they've\", 'decided'): 1, ('decided', 'its'): 1, ('its', 'more'): 4, ('more', 'fun'): 8}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def process_file_incrementally(file_path, n):\n",
    "    \"\"\"\n",
    "    Process the file line by line and update n-gram counts.\n",
    "    \"\"\"\n",
    "    n_grams = defaultdict(int)\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            tokens = line.lower().split()\n",
    "            tokens = ['<s>'] * (n-1) + tokens + ['</s>']\n",
    "            for i in range(len(tokens) - n + 1):\n",
    "                n_gram = tuple(tokens[i:i+n])\n",
    "                n_grams[n_gram] += 1\n",
    "    return n_grams\n",
    "\n",
    "# Example usage\n",
    "file_path = \"C:\\\\Users\\\\hp\\\\id2\\\\NLP\\\\tp2 donnees\\\\PA_files\\\\big_data.txt\"\n",
    "n = 2  # For bigrams\n",
    "n_gram_counts = process_file_incrementally(file_path, n)\n",
    "print(dict(list(n_gram_counts.items())[:50]))  # Print first 10 n-gram counts for inspection\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4bce3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 'see', the next word might be 'you'.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def predict_next_word(previous_word, n_gram_counts, n=2):\n",
    "    \"\"\"\n",
    "    Predict the next word based on the previous word(s) using the n-gram model.\n",
    "    \n",
    "    Args:\n",
    "    previous_word (str): The previous word or words (context).\n",
    "    n_gram_counts (dict): A dictionary of n-gram counts.\n",
    "    n (int): The order of the n-gram model.\n",
    "\n",
    "    Returns:\n",
    "    str: The predicted next word.\n",
    "    \"\"\"\n",
    "    # Create a list to store possible next words along with their probabilities\n",
    "    candidates = []\n",
    "    total_count = 0\n",
    "    \n",
    "    # Find all bigrams starting with the previous word\n",
    "    for n_gram in n_gram_counts:\n",
    "        if n_gram[0] == previous_word:\n",
    "            candidates.append((n_gram[1], n_gram_counts[n_gram]))\n",
    "            total_count += n_gram_counts[n_gram]\n",
    "    \n",
    "    if not candidates:\n",
    "        return \"No continuation found.\"\n",
    "    \n",
    "    # Normalize the counts to probabilities\n",
    "    probabilities = [float(count) / total_count for _, count in candidates]\n",
    "    \n",
    "    # Choose a next word based on the probabilities\n",
    "    next_word = random.choices([word for word, _ in candidates], weights=probabilities, k=1)[0]\n",
    "    return next_word\n",
    "\n",
    "# Example usage with a given previous word\n",
    "previous_word = \"see\"\n",
    "predicted_word = predict_next_word(previous_word, n_gram_counts)\n",
    "print(f\"After '{previous_word}', the next word might be '{predicted_word}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9ae5450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 'black', the next word might be 'cat.'.\n"
     ]
    }
   ],
   "source": [
    "previous_word = \"black\"\n",
    "predicted_word = predict_next_word(previous_word, n_gram_counts)\n",
    "print(f\"After '{previous_word}', the next word might be '{predicted_word}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56e0055f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 'blue', the next word might be 'suits,'.\n"
     ]
    }
   ],
   "source": [
    "previous_word = \"blue\"\n",
    "predicted_word = predict_next_word(previous_word, n_gram_counts)\n",
    "print(f\"After '{previous_word}', the next word might be '{predicted_word}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e39876b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e7bc3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
